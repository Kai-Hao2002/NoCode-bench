# agent_core/models.py
from django.db import models

class EvaluationTask(models.Model):
    STATUS_CHOICES = [
        ('PENDING', 'Pending'),
        ('RUNNING', 'Running'),
        ('COMPLETED', 'Completed'),
        ('FAILED', 'Failed'),
        ('FAILED_APPLY', 'Failed_Apply'), 
        ('FAILED_TEST', 'Failed_Test'),      
    ]

    base_task_id = models.CharField(max_length=255, null=True, blank=True, help_text="The base nocode_bench_id for demo tasks.")
    nocode_bench_id = models.CharField(max_length=255, unique=True, help_text="e.g. example-repo/task-001")
    doc_change_input = models.TextField(help_text="The documentation change instruction.")
    
    ground_truth_patch = models.TextField(help_text="The ground-truth diff patch from the dataset.", null=True, blank=True)

    feature_test_patch = models.TextField(help_text="The ground-truth test patch (from 'test_patch').")
    f2p_test_names = models.JSONField(default=list, help_text="List of FAIL2PASS test names.")
    p2p_test_names = models.JSONField(default=list, help_text="List of PASS2PASS test names (regression tests).")
    
    status = models.CharField(max_length=20, choices=STATUS_CHOICES, default='PENDING')
    celery_task_id = models.CharField(max_length=255, null=True, blank=True)
    
    start_time = models.DateTimeField(auto_now_add=True)
    end_time = models.DateTimeField(null=True, blank=True)
    error_details = models.TextField(null=True, blank=True)

    repo = models.CharField(max_length=255, help_text="e.g. django/django", null=True)
    version = models.CharField(max_length=50, help_text="e.g. 3.2", null=True)
    base_commit = models.CharField(max_length=100, help_text="Git SHA", null=True)

    def __str__(self):
        return f"Task: {self.nocode_bench_id} - {self.status}"

class EvaluationAttempt(models.Model):
    STATUS_CHOICES = [
        ('APPLY_FAILED', 'Apply Failed'), # AI format error
        ('TEST_FAILED', 'Test Failed'),   # Code logic error
        ('PASSED', 'Passed'),             # Tests Passed
    ]
    
    task = models.ForeignKey(EvaluationTask, on_delete=models.CASCADE, related_name='attempts')
    attempt_number = models.IntegerField()
    status = models.CharField(max_length=20, choices=STATUS_CHOICES)
    
    prompt_text = models.TextField(help_text="Full prompt sent to LLM")
    raw_response = models.TextField(help_text="Original response from LLM")
    generated_patch = models.TextField(help_text="The git diff generated in this attempt")
    test_output = models.TextField(help_text="Pytest output logs")
    
    timestamp = models.DateTimeField(auto_now_add=True)

    class Meta:
        ordering = ['attempt_number']

    def __str__(self):
        return f"Attempt {self.attempt_number} for {self.task.nocode_bench_id} - {self.status}"


class EvaluationResult(models.Model):
    task = models.OneToOneField(EvaluationTask, on_delete=models.CASCADE, related_name='result')
    
    # Required Metrics
    success_percent = models.FloatField(default=0.0)  # (CHANGE: Are new feature tests (F2P) 100% passed?)                                              
    applied_percent = models.FloatField(default=0.0)
    rt_percent = models.FloatField(default=0.0)        # (CHANGE: Are regression tests 100% passed?)
                                                     
    
    # (The paper's FV-Macro (per-instance F2P pass rate))
    fv_macro = models.FloatField(default=0.0)
    
    file_percent = models.FloatField(default=0.0)    
                                                      
    num_token = models.IntegerField(default=0)
    

    # (NEW: Optional metrics and FV calculation fields)
    run_time_seconds = models.FloatField(default=0.0) # Runtime
    f2p_passed_count = models.IntegerField(default=0) # (FV-Micro/Macro)
    f2p_total_count = models.IntegerField(default=0)  # (FV-Micro/Macro)

    p2p_passed_count = models.IntegerField(default=0)
    p2p_total_count = models.IntegerField(default=0)

    # Output
    generated_patch = models.TextField(help_text="The code patch generated by the LLM.")
