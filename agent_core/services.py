# agent_core/services.py
import os
import sys
import shutil
import subprocess
import time
import re
import stat
import platform
import json
from google import generativeai as genai
from google.generativeai.types import GenerationConfig
from django.conf import settings
from unidiff import PatchSet
from io import StringIO

# --- æ ¸å¿ƒè¨­å®š (Core Configuration) ---
ROOT_WORKSPACE = os.path.join(settings.BASE_DIR, 'nocode_workspaces')
os.makedirs(ROOT_WORKSPACE, exist_ok=True)
ORIGINAL_DATASET_ROOT = os.path.join(settings.BASE_DIR, 'NoCode-bench_Verified', 'data')


# --- æ¬Šé™éŒ¯èª¤è™•ç† (Permission Error Handler) ---
def onerror(func, path, exc_info):
    if not os.access(path, os.W_OK):
        os.chmod(path, stat.S_IWUSR | stat.S_IWRITE)
        func(path)
    else:
        raise
        
# ğŸš€ æ–°å¢ (NEW): ç”¨æ–¼æ‡‰ç”¨è£œä¸çš„è¼”åŠ©å‡½æ•¸
# (Helper function for applying patches)
def _apply_patch(workspace_path: str, patch_str: str) -> tuple[bool, str | None]:
    """
    å°‡ä¸€å€‹è£œä¸å­—ç¬¦ä¸²æ‡‰ç”¨åˆ° Git å€‰åº«ã€‚
    """
    if not patch_str:
        return False, "Warning: Empty patch string provided."
    
    # å˜—è©¦ 1: æ¨™æº–æ‡‰ç”¨ (Standard apply)
    # å˜—è©¦ 2: å¿½ç•¥ç©ºç™½èˆ‡æ›è¡Œç¬¦ (Ignore whitespace and newlines - CRITICAL FOR WINDOWS)
    # å˜—è©¦ 3: å¿½ç•¥ä¸Šä¸‹æ–‡ä¸åŒ¹é… (Recalculate context - use with caution)
    
    commands_to_try = [
        ['git', 'apply', '--ignore-whitespace', '--verbose'],
        ['git', 'apply', '--ignore-space-change', '--ignore-whitespace', '--verbose'],
        ['git', 'apply', '--recount', '--unidiff-zero', '--ignore-whitespace', '--verbose'] # å¼·åŠ›æ¨¡å¼
    ]

    last_error = ""

    for cmd in commands_to_try:
        try:
            result = subprocess.run(
                cmd,
                input=patch_str,
                cwd=workspace_path,
                text=True,
                check=False,
                capture_output=True,
                encoding='utf-8' # ç¢ºä¿ç·¨ç¢¼æ­£ç¢º
            )
            if result.returncode == 0:
                return True, None
            
            last_error = result.stderr
        except Exception as e:
            last_error = str(e)

    # å¦‚æœæ‰€æœ‰å˜—è©¦éƒ½å¤±æ•—
    error_msg = f"git apply failed after multiple attempts. Last error: {last_error}"
    print(f"ERROR: {error_msg}")
    return False, error_msg


# --- è¼”åŠ©å‡½æ•¸ (Helper Functions) ---

def setup_workspace(nocode_bench_id: str) -> str:
    """
    (æ­¤å‡½æ•¸ä¿æŒä¸è®Š)
    (This function is unchanged)
    """
    parts = nocode_bench_id.split('__')
    repo_owner = parts[0]
    match = re.match(r'^(.*?)-(\d+)$', parts[1])
    if match:
        repo_name_base = match.group(1) # e.g., 'scikit-learn', 'matplotlib'
    else:
        # å¦‚æœ regex åŒ¹é…å¤±æ•—ï¼Œé€€å›åˆ°èˆŠçš„ï¼ˆå¯èƒ½æœ‰ç¼ºé™·çš„ï¼‰é‚è¼¯
        repo_name_base = parts[1].split('-')[0]
    repo_path_segment = os.path.join(repo_owner, repo_name_base)
    original_repo_path = os.path.join(ORIGINAL_DATASET_ROOT, repo_path_segment)
    run_id = str(time.time()).replace('.', '')
    temp_dir = os.path.join(ROOT_WORKSPACE, f'run_{nocode_bench_id.replace("__", "_")}_{run_id}')
    
    if not os.path.exists(original_repo_path):
        raise FileNotFoundError(f"Original codebase not found! Check path: {original_repo_path}")
    
    try:
        shutil.copytree(original_repo_path, temp_dir)
        subprocess.run(['git', 'init'], cwd=temp_dir, check=True, capture_output=True, text=True, encoding='utf-8')
        subprocess.run(['git', 'add', '.'], cwd=temp_dir, check=True, capture_output=True, text=True, encoding='utf-8')
        subprocess.run(['git', 'commit', '-m', 'Initial snapshot', '--allow-empty'], cwd=temp_dir, check=True, capture_output=True, text=True, encoding='utf-8')
        return temp_dir
    except subprocess.CalledProcessError as e:
        raise IOError(f"Failed to initialize Git: {e.stderr}")
    except Exception as e:
        raise IOError(f"File operation failed: {e}")


def _run_tests_in_workspace(
    workspace_path: str, 
    feature_test_patch: str, 
    f2p_test_names: list[str], 
    p2p_test_names: list[str]
) -> tuple[int, int, int, int, str]: # ğŸš€ æ›´æ”¹ (CHANGE): è¿”å› 4 å€‹è¨ˆæ•¸å™¨
    """
    ğŸš€ æ›´æ”¹ (CHANGE): 
    æ­¤å‡½æ•¸ç¾åœ¨é‹è¡Œ *æ‰€æœ‰* æ¸¬è©¦ä¸€æ¬¡ï¼Œä¸¦å¾ä¸€å€‹ JSON å ±å‘Šä¸­è§£æ F2P å’Œ P2P çš„è¨ˆæ•¸ã€‚
    é€™è§£æ±ºäº† WinError 206ï¼ˆæª”åå¤ªé•·ï¼‰çš„å•é¡Œã€‚
    """
    venv_path = os.path.join(workspace_path, 'venv')
    
    if platform.system() == "Windows":
        python_executable = os.path.join(venv_path, 'Scripts', 'python.exe')
        pip_executable = os.path.join(venv_path, 'Scripts', 'pip.exe')
    else:
        python_executable = os.path.join(venv_path, 'bin', 'python')
        pip_executable = os.path.join(venv_path, 'bin', 'pip')

    full_log = []
    
    # åˆå§‹åŒ–æ‰€æœ‰ 4 å€‹è¨ˆæ•¸å™¨
    f2p_passed_count = 0
    f2p_total_count = len(f2p_test_names)
    p2p_passed_count = 0
    p2p_total_count = len(p2p_test_names)
    
    try:
        # --- æ­¥é©Ÿ 1-3ï¼šå®‰è£ (èˆ‡ä¹‹å‰ç›¸åŒ) ---
        
        # 1. å‰µå»º Venv
        # (æˆ‘å€‘ä¿ç•™ Python 3.9/3.8 çš„å›é€€é‚è¼¯ï¼Œä»¥è§£æ±ºä¾è³´åœ°ç„)
        print("Creating venv...")
        python_exec_to_try = ['python3.9', 'python3.8', sys.executable]
        venv_created = False
        log_stdout = ""
        log_stderr = ""
        
        for py_exec in python_exec_to_try:
            print(f"Attempting to create venv with {py_exec}...")
            full_log.append(f"--- Venv Creation (Attempt: {py_exec}) ---")
            try:
                result = subprocess.run(
                    [py_exec, '-m', 'venv', venv_path], 
                    cwd=workspace_path, capture_output=True, check=True,
                    text=True, encoding='utf-8', errors='replace'
                )
                log_stdout = result.stdout
                log_stderr = result.stderr
                full_log.append(f"{log_stdout}\n{log_stderr}")
                venv_created = True
                print(f"Successfully created venv with {py_exec}.")
                break 
            except (subprocess.CalledProcessError, FileNotFoundError) as e:
                log_stderr = str(e)
                full_log.append(f"Failed to create venv with {py_exec}: {log_stderr}")
        
        if not venv_created:
            return 0, f2p_total_count, 0, p2p_total_count, f"Failed to create venv. Last error: {log_stderr}"

        # 1a. å®‰è£æ ¸å¿ƒæ¸¬è©¦å¥—ä»¶
        print("Installing modern test dependencies (pytest, trustme, pytest-json-report, setuptools)...")
        deps_to_install = ['pytest', 'trustme', 'pytest-json-report', 'setuptools']
        install_cmd = [pip_executable, 'install'] + deps_to_install
        result = subprocess.run(install_cmd, cwd=workspace_path, capture_output=True, check=False)
        log_stdout = result.stdout.decode('utf-8', errors='replace')
        log_stderr = result.stderr.decode('utf-8', errors='replace')
        full_log.append(f"--- Dependency Installation (Step 1/3) ---\n{log_stdout}\n{log_stderr}")
        if result.returncode != 0:
            full_log.append("FATAL: Step 1/3 failed, aborting test run.")
            return 0, f2p_total_count, 0, p2p_total_count, "\n".join(full_log)
            

        # 2. å®‰è£å°ˆæ¡ˆçš„æ¸¬è©¦ä¾è³´é … (os.walk)
        print("Searching for project-specific test requirements...")
        dev_req_files_set = set(['requirements-dev.txt','requirements.txt','rtd_requirements.txt','requirements_test_min.txt','requirements_test_pre_commit.txt','requirements_test.txt', 'test-requirements.txt', 'requirements-tests.txt', 'dev-requirements.txt'])
        found_dev_req = False
        for root, dirs, files in os.walk(workspace_path):
            if '.git' in dirs: dirs.remove('.git')
            if 'venv' in dirs: dirs.remove('venv')
            if found_dev_req: break
            for file_name in files:
                if file_name in dev_req_files_set:
                    req_path = os.path.join(root, file_name)
                    found_dev_req = True
                    rel_req_path = os.path.relpath(req_path, workspace_path)
                    print(f"Found {rel_req_path}. Installing test dependencies...")
                    install_cmd_dev = [pip_executable, 'install', '-r', req_path]
                    result_dev = subprocess.run(install_cmd_dev, cwd=workspace_path, capture_output=True, check=False)
                    log_stdout_dev = result_dev.stdout.decode('utf-8', errors='replace')
                    log_stderr_dev = result_dev.stderr.decode('utf-8', errors='replace')
                    full_log.append(f"--- Dependency Installation (Step 2/3: {rel_req_path}) ---\n{log_stdout_dev}\n{log_stderr_dev}")
                    if result_dev.returncode != 0:
                        print(f"WARNING: Failed to install some dependencies from {rel_req_path}. {log_stderr_dev}")
                        full_log.append(f"WARNING: Installation of {rel_req_path} failed. This may or may not be critical.")
                    break
        
        if not found_dev_req:
            print("No project-specific test requirement files found. Proceeding.")
            full_log.append("--- Dependency Installation (Step 2/3) ---\nNo project-specific test requirements file found.")
            
        # 3. å®‰è£å°ˆæ¡ˆæœ¬èº«
        if os.path.exists(os.path.join(workspace_path, 'setup.py')):
            print("Found setup.py. Installing package in editable mode...")
            install_cmd_no_test = [pip_executable, 'install', '-e .']
            result_no_test = subprocess.run(install_cmd_no_test, cwd=workspace_path, capture_output=True, check=False)
            log_stdout = result_no_test.stdout.decode('utf-8', errors='replace')
            log_stderr = result_no_test.stderr.decode('utf-8', errors='replace')
            full_log.append(f"--- Dependency Installation (Step 3/3) ---\n{log_stdout}\n{log_stderr}")
            if result_no_test.returncode != 0:
                 print(f"WARNING: Fallback 'pip install -e .' failed. {result_no_test.stderr}")

        # 4. æ‡‰ç”¨ 'test_patch'
        print(f"Applying ground-truth test patch...")
        success, error_msg = _apply_patch(workspace_path, feature_test_patch)
        if not success:
             log_message = f"FATAL: Failed to apply ground-truth test patch (test_patch).\nDetails: {error_msg}"
             full_log.append(log_message)
             return 0, f2p_total_count, 0, p2p_total_count, "\n".join(full_log)

        # --- æ­¥é©Ÿ 5ï¼šé‹è¡Œæ‰€æœ‰æ¸¬è©¦ (æ–°) ---
        
        print(f"Running pytest (All tests) with JSON report...")
        report_file = os.path.join(workspace_path, 'combined_report.json')
        
        # ğŸš€ æ›´æ”¹ (CHANGE): æˆ‘å€‘åªé‹è¡Œ 'pytest'ï¼Œä¸å‚³éä»»ä½•å–®ç¨çš„æ¸¬è©¦åç¨±ã€‚
        # é€™é¿å…äº† WinError 206ã€‚
        pytest_cmd = [python_executable, '-m', 'pytest', '--json-report', f'--json-report-file={report_file}']
        
        # (æˆ‘å€‘ä½¿ç”¨ 600 ç§’ (10 åˆ†é˜) çš„ timeout)
        result_all = subprocess.run(pytest_cmd, cwd=workspace_path, capture_output=True, check=False, timeout=600)
        log_stdout = result_all.stdout.decode('utf-8', errors='replace')
        log_stderr = result_all.stderr.decode('utf-8', errors='replace')
        full_log.append(f"--- Pytest Execution (Combined) ---\n{log_stdout}\n{log_stderr}")
        
        # --- æ­¥é©Ÿ 6ï¼šè§£æçµ„åˆå ±å‘Š (æ–°) ---
        
        try:
            with open(report_file, 'r') as f:
                report = json.load(f)
            
            # å‰µå»ºå¿«é€ŸæŸ¥æ‰¾é›†åˆ
            f2p_set = set(f2p_test_names)
            p2p_set = set(p2p_test_names)
            
            if 'tests' in report:
                for test in report['tests']:
                    nodeid = test.get('nodeid')
                    outcome = test.get('outcome')
                    
                    if outcome == 'passed':
                        if nodeid in f2p_set:
                            f2p_passed_count += 1
                        elif nodeid in p2p_set:
                            p2p_passed_count += 1
            
            print(f"Feature test results: {f2p_passed_count} / {f2p_total_count} passed.")
            print(f"Regression test results: {p2p_passed_count} / {p2p_total_count} passed.")
            
        except Exception as e:
            print(f"ERROR: Could not parse {report_file}: {e}")
            full_log.append(f"ERROR: Could not parse {report_file}: {e}")

        # 7. è¿”å›æ‰€æœ‰ 4 å€‹è¨ˆæ•¸å™¨
        return f2p_passed_count, f2p_total_count, p2p_passed_count, p2p_total_count, "\n".join(full_log)

    except subprocess.TimeoutExpired:
        full_log.append("--- Pytest Execution ---\nERROR: Pytest timed out after 600 seconds.")
        return f2p_passed_count, f2p_total_count, p2p_passed_count, p2p_total_count, "\n".join(full_log)
    except Exception as e:
        full_log.append(f"--- Testing Framework Error ---\nAn unexpected error occurred: {e}")
        return f2p_passed_count, f2p_total_count, p2p_passed_count, p2p_total_count, "\n".join(full_log)


def _get_relevant_files_from_llm(model, doc_change: str, workspace_path: str) -> list[str]:
    """
    (æ­¤å‡½æ•¸ä¿æŒä¸è®Š)
    """
    all_files = []
    # (os.walk è¿´åœˆä¿æŒä¸è®Š)
    for root, _, files in os.walk(workspace_path):
        if '.git' in root or 'docs' in root or '.venv' in root or 'venv' in root: continue
        for file in files:
            if file.endswith(('.py', '.html', '.css', '.js', 'setup.py', 'requirements.txt')):
                rel_path = os.path.relpath(os.path.join(root, file), workspace_path)
                all_files.append(rel_path.replace('\\', '/'))
    if not all_files:
        print(f"[Task] WARNING: os.walk found NO files in {workspace_path}")
        return []
    file_list_str = ', '.join(all_files).replace('\\', '/')
    if not file_list_str:
        print(f"[Task] WARNING: No code files found to analyze.")
        return []

    # ğŸš€ é€™æ˜¯æ–°çš„ã€æ›´æ™ºæ…§çš„æç¤ºè©
    # In agent_core/services.py

    prompt = (
        f"You are an expert file locator agent. Your goal is to identify ALL files required for a code change, AND files that might break due to side effects.\n\n"
        f"**DOCUMENTATION CHANGE:**\n{doc_change}\n\n"
        f"**CODE FILE LIST:**\n{file_list_str}\n\n"
        f"**THINKING PROCESS:**\n"
        "1.  **Core Logic:** Where is the primary code change? (e.g., 'utils.py')\n"
        "2.  **Impact Analysis (CRITICAL FOR REGRESSION):** Who IMPORTS or USES the code from step 1? If you modify a shared function, you MUST inspect the files that call it to ensure backward compatibility.\n" # <--- æ–°å¢é€™è¡Œ (Added this)
        "3.  **Dependencies:** Check `compat.py` and `__init__.py`.\n"
        "4.  **Selection:** List the files to modify AND the files to read for context.\n\n"
        f"**INSTRUCTIONS:**\n"
        "1.  Respond ONLY with a JSON object: {{\"files\": [\"path/to/mod.py\", \"path/to/caller.py\"]}}\n"
        "2.  It is better to include a few extra 'caller' files to prevent regression bugs than to miss them.\n" # <--- é¼“å‹µå¤šé¸ (Encourage slightly lower precision for better context)
    )
    response_text = None
    try:
        response = model.generate_content(
            prompt,
            generation_config=GenerationConfig(
                response_mime_type="application/json"
            )
        )
        # (å‡½æ•¸çš„å…¶é¤˜éƒ¨åˆ†ä¿æŒä¸è®Š)
        response_text = response.text
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if not json_match:
            print(f"[Task] ERROR: AI response did not contain a JSON object. Response: {response_text}")
            return []
        json_text = json_match.group(0)
        print(f"[Task] DEBUG: File finder LLM response (extracted):\n{json_text}")
        data = json.loads(json_text)
        if "files" not in data or not isinstance(data["files"], list):
            print(f"[Task] ERROR: AI response JSON was in wrong format: {json_text}")
            return []
        llm_files = data["files"]
        valid_files = [f.strip().replace('\\', '/') for f in llm_files if f.strip() in all_files]
        if not valid_files and llm_files:
             print(f"[Task] WARNING: AI found files {llm_files}, but none were in the master 'all_files' list.")
        return valid_files
    except json.JSONDecodeError:
        print(f"[Task] ERROR: AI response was not valid JSON: {response_text}")
        return []
    except Exception as e:
        print(f"[Task] ERROR: Failed to parse AI file list: {e}\nResponse text: {response_text}")
        return []

def _get_file_contexts(workspace_path: str, relevant_files: list[str]) -> str:
    """
    (æ­¤å‡½æ•¸ä¿æŒä¸è®Š)
    (This function is unchanged)
    """
    context_prompt_parts = []
    # ... (æ­¤å‡½æ•¸çš„å…¶é¤˜éƒ¨åˆ†ä¿æŒä¸è®Š) ...
    for file_path in relevant_files:
        full_path = os.path.join(workspace_path, file_path)
        if not os.path.exists(full_path):
            print(f"WARNING: File `{file_path}` identified by AI does not exist. Skipping.")
            continue
        try:
            with open(full_path, 'r', encoding='utf-8', errors='ignore') as f:
                file_content = f.read()
            context_prompt_parts.append(
                f"--- START OF FILE: {file_path} ---\n"
                f"{file_content}\n"
                f"--- END OF FILE: {file_path} ---\n"
            )
        except Exception as e:
            print(f"Error reading file {file_path}: {e}")
    return "\n".join(context_prompt_parts)

def _parse_v7_response(raw_response_text: str) -> dict[str, str]:
    """
    (æ­¤å‡½æ•¸ä¿æŒä¸è®Š)
    (This function is unchanged)
    """
    modified_files = {}
    # ... (æ­¤å‡½æ•¸çš„å…¶é¤˜éƒ¨åˆ†ä¿æŒä¸è®Š) ...
    file_chunks = re.split(r'--- START OF FILE: (.*?) ---\n', raw_response_text)
    if len(file_chunks) < 2:
        raise ValueError("AI response did not contain any '--- START OF FILE: ' delimiters.")
    for i in range(1, len(file_chunks), 2):
        file_path = file_chunks[i].strip()
        content_part = file_chunks[i+1]
        content = re.sub(r'--- END OF FILE: .*? ---', '', content_part, flags=re.DOTALL).strip()
        if file_path and content:
            modified_files[file_path] = content
        else:
            print(f"WARNING: Could not parse file chunk: Filepath='{file_path}', Content preview='{content[:50]}...'")
    if not modified_files:
        raise ValueError("AI response was parsed, but no valid file content blocks were found.")
    return modified_files


# --- æŒ‡æ¨™è¨ˆç®— (Metrics Calculation) ---
# (parse_patch å’Œ calculate_f1_score ä¿æŒä¸è®Š)
def parse_patch(patch_str: str) -> dict[str, set[int]]:
    if not patch_str: return {}
    try:
        patch = PatchSet(StringIO(patch_str))
        changed_files = {}
        for patched_file in patch:
            if patched_file.is_binary_file: continue
            file_path = patched_file.target_file[2:] if patched_file.target_file.startswith('b/') else patched_file.target_file
            changed_lines = set()
            for hunk in patched_file:
                for line in hunk:
                    if line.is_added:
                        changed_lines.add(line.target_line_no)
                    elif line.is_removed:
                        changed_lines.add(line.source_line_no)
            if changed_lines:
                changed_files[file_path] = changed_lines
        return changed_files
    except Exception as e:
        print(f"Error parsing patch: {e}\nPatch content:\n{patch_str[:500]}...")
        return {}

def calculate_f1_score(pred_set: set, gold_set: set) -> float:
    if not gold_set: return 1.0 if not pred_set else 0.0
    if not pred_set: return 0.0
    tp = len(pred_set.intersection(gold_set))
    fp = len(pred_set.difference(gold_set))
    fn = len(gold_set.difference(pred_set))
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    return f1

def calculate_all_metrics(
    f2p_passed_count: int,
    f2p_total_count: int,
    # ğŸš€ æ–°å¢ (NEW): P2P è¨ˆæ•¸å™¨
    p2p_passed_count: int,
    p2p_total_count: int,
    regression_tests_passed: bool, # (æˆ‘å€‘ä»ç„¶æ¥å—é€™å€‹ï¼Œä½†æœƒå¿½ç•¥å®ƒ)
    applied_successfully: bool, 
    generated_patch: str, 
    ground_truth_patch: str, 
    run_time_seconds: float
) -> dict:
    """
    ğŸš€ æ›´æ”¹ (CHANGE): æ­¤å‡½æ•¸ç¾åœ¨æ¥å— P2P è¨ˆæ•¸å™¨ä¸¦è¨ˆç®— RT% ç™¾åˆ†æ¯”ã€‚
    """
    
    # 1. Success% å’Œ RT%
    success_percent = 100.0 if (f2p_passed_count == f2p_total_count and f2p_total_count > 0) else 0.0
    applied_percent = 100.0 if applied_successfully else 0.0
    
    # ğŸš€ æ›´æ”¹ (CHANGE): RT% ç¾åœ¨æ˜¯ P2P æ¸¬è©¦çš„ç™¾åˆ†æ¯”
    # (å¦‚æœæ²’æœ‰ P2P æ¸¬è©¦ï¼Œå‰‡ RT% ç‚º 100%)
    rt_percent = 100.0 * (p2p_passed_count / p2p_total_count) if p2p_total_count > 0 else 100.0
    
    # 2. FV-Macro (æ¯å€‹å¯¦ä¾‹)
    fv_macro = 100.0 * (f2p_passed_count / f2p_total_count) if f2p_total_count > 0 else 0.0

    # 3. File% (ç²¾ç¢ºç‡)
    pred_files_lines = parse_patch(generated_patch)
    gold_files_lines = parse_patch(ground_truth_patch)
    pred_file_set = set(pred_files_lines.keys())
    gold_file_set = set(gold_files_lines.keys())
    
    file_intersection = len(pred_file_set.intersection(gold_file_set))
    if len(pred_file_set) == 0:
        file_percent = 100.0 if len(gold_file_set) == 0 else 0.0
    else:
        file_percent = (file_intersection / len(pred_file_set)) * 100.0

    return {
        'success_percent': success_percent,
        'applied_percent': applied_percent,
        'rt_percent': rt_percent, # ğŸš€ ç¾åœ¨æ˜¯ç™¾åˆ†æ¯”
        'fv_macro': fv_macro,
        'file_percent': file_percent,
        'num_token': len(generated_patch.split()),
        'run_time_seconds': run_time_seconds,
        'f2p_passed_count': f2p_passed_count,
        'f2p_total_count': f2p_total_count,
        'p2p_passed_count': p2p_passed_count, # ğŸš€ æ–°å¢ (NEW)
        'p2p_total_count': p2p_total_count,   # ğŸš€ æ–°å¢ (NEW)
    }

# --- æ ¸å¿ƒ Agent å·¥ä½œå‡½æ•¸ (Core Agent Worker Function) ---

def run_agent_attempt(
    workspace_path: str, 
    model, 
    prompt_text: str, 
    feature_test_patch: str,
    f2p_test_names: list[str],
    p2p_test_names: list[str]
) -> dict:
    """
    åŸ·è¡Œä¸€æ¬¡ Agent å˜—è©¦ï¼šç”Ÿæˆä»£ç¢¼ -> æ‡‰ç”¨ -> æ¸¬è©¦ã€‚
    (Executes one Agent attempt: Generate Code -> Apply -> Test.)
    
    ğŸš€ é‡å¤§ä¿®æ”¹ (MAJOR CHANGE): 
    ç¾åœ¨å¯¦æ–½ã€Œåš´æ ¼é€šéæ¨™æº–ã€(Strict Passing Criteria)ã€‚
    åªæœ‰ç•¶ F2P (æ–°åŠŸèƒ½) å’Œ P2P (èˆŠåŠŸèƒ½) å…¨éƒ¨é€šéæ™‚ï¼Œæ‰è¦–ç‚ºæˆåŠŸã€‚
    (Now enforces Strict Passing Criteria. Only considered successful if BOTH F2P and P2P pass.)
    """
    
    raw_response_text = ""
    final_patch_str = ""
    test_output = ""
    
    try:
        # é‡ç½®å·¥ä½œå€ (Reset workspace)
        subprocess.run(['git', 'reset', '--hard', 'HEAD'], cwd=workspace_path, capture_output=True, text=True, check=True)
        
        # 1. ç”Ÿæˆç¨‹å¼ç¢¼ (Generate Code)
        response = model.generate_content(prompt_text)
        raw_response_text = response.text
        
        # 2. è§£æå›æ‡‰ (Parse Response)
        try:
            modified_files = _parse_v7_response(raw_response_text)
        except Exception as e:
            print(f"ERROR: AI response parsing failed: {e}\nRaw Response: {raw_response_text[:1000]}")
            return {
                'status': 'APPLY_FAILED', 'error': f"AI response parsing failed: {e}",
                'patch': '', 'raw_response': raw_response_text,
                'f2p_passed_count': 0, 'f2p_total_count': 0, 
                'p2p_passed_count': 0, 'p2p_total_count': 0,
                'regression_tests_passed': False
            }

        # 3. å°‡æ–°å…§å®¹å¯«å…¥æ–‡ä»¶ (Write new contents to files)
        for file_path, new_content in modified_files.items():
            try:
                if '..' in file_path: continue
                full_path = os.path.join(workspace_path, file_path)
                os.makedirs(os.path.dirname(full_path), exist_ok=True)
                with open(full_path, 'w', encoding='utf-8') as f:
                    f.write(new_content)
            except Exception as e:
                 return {
                    'status': 'APPLY_FAILED', 'error': f"Failed to write file {file_path} to disk: {e}",
                    'patch': '', 'raw_response': raw_response_text,
                    'f2p_passed_count': 0, 'f2p_total_count': 0, 
                    'p2p_passed_count': 0, 'p2p_total_count': 0,
                    'regression_tests_passed': False
                }

        # 4. ç”Ÿæˆè£œä¸ (Generate Patch)
        diff_result = subprocess.run(
            ['git', 'diff', '--no-prefix'], 
            cwd=workspace_path, capture_output=True, text=True, check=True, encoding='utf-8'
        )
        final_patch_str = diff_result.stdout
        
        if not final_patch_str:
             return {
                'status': 'TEST_FAILED', 'patch': '',
                'test_output': 'AI agent produced no code changes.',
                'raw_response': raw_response_text,
                'f2p_passed_count': 0, 'f2p_total_count': 0,
                'p2p_passed_count': 0, 'p2p_total_count': 0,
                'regression_tests_passed': False,
            }

        # 5. é‹è¡Œçµ„åˆæ¸¬è©¦ä¸¦æ•ç² 4 å€‹è¨ˆæ•¸å™¨ (Run combined tests and capture 4 counters)
        f2p_passed_count, f2p_total_count, p2p_passed_count, p2p_total_count, test_output = _run_tests_in_workspace(
            workspace_path, 
            feature_test_patch,
            f2p_test_names,
            p2p_test_names
        )
        
        # --- ğŸš€ é—œéµä¿®æ”¹é‚è¼¯ (CRITICAL MODIFIED LOGIC) ---
        
        # åˆ¤æ–· F2P æ˜¯å¦å…¨é (Check if all Feature tests passed)
        feature_tests_passed = (f2p_passed_count == f2p_total_count) if f2p_total_count > 0 else False
        
        # åˆ¤æ–· P2P æ˜¯å¦å…¨é (Check if all Regression tests passed)
        # å¦‚æœæ²’æœ‰ P2P æ¸¬è©¦ (count=0)ï¼Œé è¨­è¦–ç‚ºé€šé
        regression_tests_passed = (p2p_passed_count == p2p_total_count) if p2p_total_count > 0 else True
        
        # åªæœ‰ç•¶ "å…©è€…çš†ç‚º True" æ™‚ï¼Œæ‰ç®—ä»»å‹™æˆåŠŸ (COMPLETED/PASSED)
        # Only consider the task successful if BOTH are True
        if feature_tests_passed and regression_tests_passed:
            status = 'PASSED'
        else:
            status = 'TEST_FAILED' 
            # æ³¨æ„ï¼šå³ä½¿ F2P é€šéäº†ï¼Œå¦‚æœ Regression å¤±æ•—ï¼Œé€™è£¡ä¹Ÿæœƒè®Šæˆ TEST_FAILEDã€‚
            # é€™æ¨£ tasks.py å°±æœƒæ•æ‰åˆ°ä¸¦é€²è¡Œé‡è©¦ã€‚
            # Note: Even if F2P passed, if Regression failed, this becomes TEST_FAILED.
            # This ensures tasks.py catches it and triggers a retry.

        return {
            'status': status, 
            'patch': final_patch_str,
            'test_output': test_output, 
            'raw_response': raw_response_text,
            'f2p_passed_count': f2p_passed_count,
            'f2p_total_count': f2p_total_count,
            'p2p_passed_count': p2p_passed_count,
            'p2p_total_count': p2p_total_count,
            'regression_tests_passed': regression_tests_passed,
        }
        
    except Exception as e:
        print(f"FATAL ERROR in run_agent_attempt: {e}")
        return {
            'status': 'APPLY_FAILED',
            'error': f"An unexpected error occurred in the agent worker: {e}",
            'patch': final_patch_str,
            'test_output': test_output,
            'raw_response': raw_response_text,
            'f2p_passed_count': 0, 'f2p_total_count': 0, 
            'p2p_passed_count': 0, 'p2p_total_count': 0,
            'regression_tests_passed': False
        }

def setup_custom_workspace(github_url: str) -> str:
    """
    (æ­¤å‡½æ•¸ä¿æŒä¸è®Š)
    """
    run_id = str(time.time()).replace('.', '')
    # ç”¢ç”Ÿä¸€å€‹å”¯ä¸€çš„ç›®éŒ„åç¨±
    repo_name = github_url.split('/')[-1].replace('.git', '')
    temp_dir = os.path.join(ROOT_WORKSPACE, f'demo_{repo_name}_{run_id}')
    
    try:
        # è¤‡è£½ Git å€‰åº«
        print(f"Cloning repo from {github_url} into {temp_dir}...")
        subprocess.run(
            ['git', 'clone', '--depth', '1', github_url, temp_dir],
            check=True, capture_output=True, text=True, encoding='utf-8'
        )
        
        # (å¯é¸ï¼Œä½†æ¨è–¦) åˆå§‹åŒ– Gitï¼Œä»¥ä¾¿æˆ‘å€‘å¯ä»¥ 'git diff'
        subprocess.run(['git', 'init'], cwd=temp_dir, check=True, capture_output=True, text=True, encoding='utf-8')
        subprocess.run(['git', 'add', '.'], cwd=temp_dir, check=True, capture_output=True, text=True, encoding='utf-8')
        subprocess.run(['git', 'commit', '-m', 'Initial snapshot', '--allow-empty'], cwd=temp_dir, check=True, capture_output=True, text=True, encoding='utf-8')
        print(f"Workspace initialized at {temp_dir}")
        return temp_dir
        
    except subprocess.CalledProcessError as e:
        raise IOError(f"Failed to clone Git repo: {e.stderr}")
    except Exception as e:
        raise IOError(f"File operation failed: {e}")

# ... (åœ¨ run_agent_attempt æ—é‚Š)

def run_agent_demo_attempt(
    workspace_path: str, 
    model, 
    prompt_text: str
) -> dict:
    """
    (æ­¤å‡½æ•¸ä¿æŒä¸è®Š)
    """
    
    raw_response_text = ""
    final_patch_str = ""
    
    try:
        # é‡ç½®å·¥ä½œå€
        subprocess.run(['git', 'reset', '--hard', 'HEAD'], cwd=workspace_path, capture_output=True, text=True, check=True)
        
        # 1. ç”Ÿæˆç¨‹å¼ç¢¼
        response = model.generate_content(prompt_text)
        raw_response_text = response.text
        
        # 2. è§£æå›æ‡‰
        try:
            modified_files = _parse_v7_response(raw_response_text)
        except Exception as e:
            print(f"ERROR: AI response parsing failed: {e}")
            return {'status': 'APPLY_FAILED', 'patch': '', 'raw_response': raw_response_text}

        # 3. å°‡æ–°å…§å®¹å¯«å…¥æ–‡ä»¶
        for file_path, new_content in modified_files.items():
            try:
                if '..' in file_path: continue
                full_path = os.path.join(workspace_path, file_path)
                os.makedirs(os.path.dirname(full_path), exist_ok=True)
                with open(full_path, 'w', encoding='utf-8') as f:
                    f.write(new_content)
            except Exception as e:
                 return {'status': 'APPLY_FAILED', 'patch': '', 'raw_response': raw_response_text}

        # 4. ç”Ÿæˆè£œä¸
        diff_result = subprocess.run(
            ['git', 'diff', '--no-prefix'], 
            cwd=workspace_path, capture_output=True, text=True, check=True, encoding='utf-8'
        )
        final_patch_str = diff_result.stdout
        
        # 5. æˆåŠŸè¿”å› (ä¸é‹è¡Œæ¸¬è©¦)
        return {
            'status': 'COMPLETED', # ç‹€æ…‹ç¸½æ˜¯ COMPLETEDï¼Œå› ç‚ºæ²’æœ‰æ¸¬è©¦
            'patch': final_patch_str,
            'raw_response': raw_response_text,
        }
        
    except Exception as e:
        print(f"FATAL ERROR in run_agent_demo_attempt: {e}")
        return {'status': 'APPLY_FAILED', 'patch': '', 'raw_response': raw_response_text}