import os
import sys
import shutil
import subprocess
import time
import re
import stat
import platform
import json
from google import generativeai as genai
from google.generativeai.types import GenerationConfig, RequestOptions
from django.conf import settings
from unidiff import PatchSet
from io import StringIO

# --- æ ¸å¿ƒè¨­å®š (Core Configuration) ---
ROOT_WORKSPACE = os.path.join(settings.BASE_DIR, 'nocode_workspaces')
os.makedirs(ROOT_WORKSPACE, exist_ok=True)
ORIGINAL_DATASET_ROOT = os.path.join(settings.BASE_DIR, 'NoCode-bench_Verified', 'data')


# --- æ¬Šé™éŒ¯èª¤è™•ç† (Permission Error Handler) ---
def onerror(func, path, exc_info):
    if not os.access(path, os.W_OK):
        os.chmod(path, stat.S_IWUSR | stat.S_IWRITE)
        func(path)
    else:
        raise


# --- æ¸¬è©¦åç¨±æ ¼å¼è½‰æ› (Test Name Format Conversion) ---
def _convert_test_name_to_pytest_format(test_name: str) -> str:
    """
    Convert test name from unittest/Django format to pytest format.
    
    Unittest/Django format: "test_name (module.path.ClassName)"
    Pytest format: "module/path.py::ClassName::test_name"
    
    If the test_name is already in pytest format (contains "::"), return it unchanged.
    """
    # Check if already in pytest format
    if '::' in test_name:
        return test_name
    
    # Try to match unittest format: "test_name (module.path.ClassName)"
    match = re.match(r'^(\w+)\s+\(([^)]+)\)$', test_name.strip())
    
    if match:
        test_method = match.group(1)
        full_class_path = match.group(2)
        
        # Split the full class path into module path and class name
        # e.g., "pagination.tests.PaginationTests" -> module: "pagination.tests", class: "PaginationTests"
        parts = full_class_path.rsplit('.', 1)
        
        if len(parts) == 2:
            module_path, class_name = parts
            # Convert module path to file path
            # e.g., "pagination.tests" -> "tests/pagination/tests.py"
            file_path = module_path.replace('.', '/') + '.py'
            
            # Return in pytest format: file_path::ClassName::test_method
            return f"{file_path}::{class_name}::{test_method}"
    
    # If conversion failed, return original
    return test_name

        
# ğŸš€ æ–°å¢ (NEW): ç”¨æ–¼æ‡‰ç”¨è£œä¸çš„è¼”åŠ©å‡½æ•¸
# (Helper function for applying patches)
def _apply_patch(workspace_path: str, patch_str: str) -> tuple[bool, str | None]:
    """
    å°‡ä¸€å€‹è£œä¸å­—ç¬¦ä¸²æ‡‰ç”¨åˆ° Git å€‰åº«ã€‚
    (Applies a patch string to a git repo.)
    """
    if not patch_str:
        msg = "Warning: Empty patch string provided to _apply_patch."
        print(msg)
        return False, msg # ğŸš€ æ›´æ”¹ (CHANGE)
    try:
        result = subprocess.run(
            ['git', 'apply', '--ignore-whitespace'],
            input=patch_str,
            cwd=workspace_path,
            text=True,
            check=False,
            capture_output=True,
            encoding='utf-8'
        )
        if result.returncode == 0:
            return True, None # ğŸš€ æ›´æ”¹ (CHANGE)
        
        # ğŸš€ æ›´æ”¹ (CHANGE): æ•ç²éŒ¯èª¤ä¸¦è¿”å›
        error_msg = f"git apply failed: {result.stderr}"
        print(f"ERROR: {error_msg}")
        return False, error_msg
        
    except Exception as e:
        # ğŸš€ æ›´æ”¹ (CHANGE): æ•ç²éŒ¯èª¤ä¸¦è¿”å›
        error_msg = f"ERROR: Exception during _apply_patch: {e}"
        print(error_msg)
        return False, error_msg


# --- è¼”åŠ©å‡½æ•¸ (Helper Functions) ---

def setup_workspace(nocode_bench_id: str) -> str:
    """
    (æ­¤å‡½æ•¸ä¿æŒä¸è®Š)
    (This function is unchanged)
    """
    parts = nocode_bench_id.split('__')
    repo_owner = parts[0]
    repo_name_base = parts[1].split('-')[0]
    repo_path_segment = os.path.join(repo_owner, repo_name_base)
    original_repo_path = os.path.join(ORIGINAL_DATASET_ROOT, repo_path_segment)
    run_id = str(time.time()).replace('.', '')
    temp_dir = os.path.join(ROOT_WORKSPACE, f'run_{nocode_bench_id.replace("__", "_")}_{run_id}')
    
    if not os.path.exists(original_repo_path):
        raise FileNotFoundError(f"Original codebase not found! Check path: {original_repo_path}")
    
    try:
        shutil.copytree(original_repo_path, temp_dir)
        subprocess.run(['git', 'init'], cwd=temp_dir, check=True, capture_output=True, text=True, encoding='utf-8')
        subprocess.run(['git', 'add', '.'], cwd=temp_dir, check=True, capture_output=True, text=True, encoding='utf-8')
        subprocess.run(['git', 'commit', '-m', 'Initial snapshot', '--allow-empty'], cwd=temp_dir, check=True, capture_output=True, text=True, encoding='utf-8')
        return temp_dir
    except subprocess.CalledProcessError as e:
        raise IOError(f"Failed to initialize Git: {e.stderr}")
    except Exception as e:
        raise IOError(f"File operation failed: {e}")


def _run_tests_in_workspace(
    workspace_path: str, 
    feature_test_patch: str, 
    f2p_test_names: list[str], 
    p2p_test_names: list[str]
) -> tuple[int, int, bool, str]:
    """
    ğŸš€ æ›´æ”¹ (CHANGE): 
    æ­¤å‡½æ•¸ç¾åœ¨æœƒè‡ªå‹•æŸ¥æ‰¾ä¸¦å®‰è£ test-requirements.txt
    (This function will now automatically find and install test-requirements.txt)
    """
    venv_path = os.path.join(workspace_path, 'venv')
    
    if platform.system() == "Windows":
        python_executable = os.path.join(venv_path, 'Scripts', 'python.exe')
        pip_executable = os.path.join(venv_path, 'Scripts', 'pip.exe')
    else:
        python_executable = os.path.join(venv_path, 'bin', 'python')
        pip_executable = os.path.join(venv_path, 'bin', 'pip')

    full_log = []
    
    f2p_passed_count = 0
    f2p_total_count = len(f2p_test_names) # ç¸½æ•¸æ˜¯ F2P åˆ—è¡¨çš„é•·åº¦
    regression_tests_passed = False
    
    try:
        # 1. å‰µå»º Venv (Create Venv)
        print(f"Creating venv at {venv_path}...")
        result = subprocess.run([sys.executable, '-m', 'venv', venv_path], cwd=workspace_path, capture_output=True, check=False)
        log_stdout = result.stdout.decode('utf-8', errors='replace')
        log_stderr = result.stderr.decode('utf-8', errors='replace')
        full_log.append(f"--- Venv Creation ---\n{log_stdout}\n{log_stderr}")
        if result.returncode != 0:
            # æ–¹æ¡ˆä¸‰ï¼šVenv åˆ›å»ºå¤±è´¥è§†ä¸ºç¯å¢ƒé”™è¯¯
            return 0, f2p_total_count, False, f"ENV_ERROR: Failed to create venv.\n{log_stderr}"

        # 2a. å®‰è£æ ¸å¿ƒæ¸¬è©¦å¥—ä»¶
        print("Installing modern test dependencies (pytest, trustme, pytest-json-report, setuptools)...")
        deps_to_install = ['pytest', 'trustme', 'pytest-json-report', 'setuptools']
        install_cmd = [pip_executable, 'install'] + deps_to_install
        result = subprocess.run(install_cmd, cwd=workspace_path, capture_output=True, check=False)
        log_stdout = result.stdout.decode('utf-8', errors='replace')
        log_stderr = result.stderr.decode('utf-8', errors='replace')
        full_log.append(f"--- Dependency Installation (Step 1/3) ---\n{log_stdout}\n{log_stderr}")
        if result.returncode != 0:
            full_log.append("FATAL: Step 1/3 (pytest installation) failed, aborting test run.")
            # æ–¹æ¡ˆä¸‰ï¼šæ ‡è®°ä¸ºç¯å¢ƒé”™è¯¯
            return 0, f2p_total_count, False, "ENV_ERROR: " + "\n".join(full_log)
            
        # ğŸš€ æ›´æ”¹ (CHANGE): æ­¥é©Ÿ 2b - ä½¿ç”¨ os.walk éæ­¸æŸ¥æ‰¾ä¾è³´æª”æ¡ˆ
        print("Searching for project-specific test requirements...")
        # (å°‡åˆ—è¡¨è½‰æ›ç‚ºé›†åˆ (Set) ä»¥åŠ å¿«æŸ¥æ‰¾é€Ÿåº¦)
        dev_req_files_set = set(['requirements-dev.txt','requirements.txt','rtd_requirements.txt','requirements_test_min.txt','requirements_test_pre_commit.txt','requirements_test.txt', 'test-requirements.txt', 'requirements-tests.txt', 'dev-requirements.txt'])
        found_dev_req = False
        for root, dirs, files in os.walk(workspace_path):
            # é¿å…æœç´¢ .git å’Œ venv ç›®éŒ„
            if '.git' in dirs: dirs.remove('.git')
            if 'venv' in dirs: dirs.remove('venv')
            
            if found_dev_req: break # æ‰¾åˆ°ä¸€å€‹å°±ç«‹å³åœæ­¢æœç´¢

            for file_name in files:
                if file_name in dev_req_files_set:
                    req_path = os.path.join(root, file_name)
                    found_dev_req = True
                    
                    # (ä½¿ç”¨ç›¸å°è·¯å¾‘é€²è¡Œæ—¥èªŒè¨˜éŒ„ï¼Œæ›´æ¸…æ™°)
                    rel_req_path = os.path.relpath(req_path, workspace_path)
                    print(f"Found {rel_req_path}. Installing test dependencies...")
                    
                    install_cmd_dev = [pip_executable, 'install', '-r', req_path]
                    result_dev = subprocess.run(install_cmd_dev, cwd=workspace_path, capture_output=True, check=False)
                    log_stdout_dev = result_dev.stdout.decode('utf-8', errors='replace')
                    log_stderr_dev = result_dev.stderr.decode('utf-8', errors='replace')
                    
                    full_log.append(f"--- Dependency Installation (Step 2/3: {rel_req_path}) ---\n{log_stdout_dev}\n{log_stderr_dev}")
                    
                    if result_dev.returncode != 0:
                        print(f"WARNING: Failed to install some dependencies from {rel_req_path}. {log_stderr_dev}")
                        full_log.append(f"WARNING: Installation of {rel_req_path} failed. This may or may not be critical.")
                        # æ–¹æ¡ˆä¸‰ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯ä¸¥é‡é”™è¯¯
                        if "error" in log_stderr_dev.lower() and "could not find" in log_stderr_dev.lower():
                            print(f"CRITICAL: Dependency installation failed critically.")
                            return 0, f2p_total_count, False, "ENV_ERROR: " + "\n".join(full_log)
                    
                    break # æ‰¾åˆ°ä¸€å€‹å°±è·³å‡ºå…§éƒ¨å¾ªç’° (files loop)
        
        if not found_dev_req:
            print("No project-specific test requirement files found. Proceeding.")
            full_log.append("--- Dependency Installation (Step 2/3) ---\nNo project-specific test requirements file found.")
            
        # 2c. å®‰è£å°ˆæ¡ˆæœ¬èº« (åŸä¾†çš„ 2b)
        if os.path.exists(os.path.join(workspace_path, 'setup.py')):
            print("Found setup.py. Installing package in editable mode...")
            install_cmd_no_test = [pip_executable, 'install', '-e .']
            result_no_test = subprocess.run(install_cmd_no_test, cwd=workspace_path, capture_output=True, check=False)
            log_stdout = result_no_test.stdout.decode('utf-8', errors='replace')
            log_stderr = result_no_test.stderr.decode('utf-8', errors='replace')
            full_log.append(f"--- Dependency Installation (Step 3/3) ---\n{log_stdout}\n{log_stderr}")
            if result_no_test.returncode != 0:
                 print(f"WARNING: Fallback 'pip install -e .' failed. {result_no_test.stderr}")

        # 3. æ‡‰ç”¨ 'test_patch'
        print(f"Applying ground-truth test patch...")
        # ğŸš€ æ›´æ”¹ (CHANGE): æ•ç²å…ƒçµ„ (tuple)
        success, error_msg = _apply_patch(workspace_path, feature_test_patch)
        
        if not success:
             # ğŸš€ æ›´æ”¹ (CHANGE): å°‡è©³ç´°éŒ¯èª¤æ·»åŠ åˆ°æ—¥èªŒä¸­
             log_message = f"FATAL: Failed to apply ground-truth test patch (test_patch).\nDetails: {error_msg}"
             full_log.append(log_message)
             return 0, f2p_total_count, False, "\n".join(full_log)

        # 4. é‹è¡Œæ¸¬è©¦ 1ï¼šF2P æ¸¬è©¦ (å¸¶ JSON å ±å‘Š)
        print(f"Running pytest (Test 1: {f2p_total_count} Feature Tests)...")
        f2p_report_file = os.path.join(workspace_path, 'f2p_report.json')
        # Convert test names to pytest format
        f2p_test_names_converted = [_convert_test_name_to_pytest_format(name) for name in f2p_test_names]
        pytest_cmd_feature = [python_executable, '-m', 'pytest', '--json-report', f'--json-report-file={f2p_report_file}'] + f2p_test_names_converted
        
        result_feature = subprocess.run(pytest_cmd_feature, cwd=workspace_path, capture_output=True, check=False, timeout=300)
        log_stdout = result_feature.stdout.decode('utf-8', errors='replace')
        log_stderr = result_feature.stderr.decode('utf-8', errors='replace')
        full_log.append(f"--- Pytest Execution (Feature Test) ---\n{log_stdout}\n{log_stderr}")
        
        try:
            with open(f2p_report_file, 'r') as f:
                report = json.load(f)
                f2p_passed_count = report.get('summary', {}).get('passed', 0)
            print(f"Feature test results: {f2p_passed_count} / {f2p_total_count} passed.")
        except Exception as e:
            print(f"ERROR: Could not parse f2p_report.json: {e}")
            full_log.append(f"ERROR: Could not parse f2p_report.json: {e}")

        # 5. é‹è¡Œæ¸¬è©¦ 2ï¼šP2P è¿´æ­¸æ¸¬è©¦
        p2p_total_count = len(p2p_test_names)
        if p2p_total_count > 0:
            print(f"Running pytest (Test 2: {p2p_total_count} Regression Tests)...")
            # Convert test names to pytest format
            p2p_test_names_converted = [_convert_test_name_to_pytest_format(name) for name in p2p_test_names]
            pytest_cmd_regression = [python_executable, '-m', 'pytest'] + p2p_test_names_converted
            result_regression = subprocess.run(pytest_cmd_regression, cwd=workspace_path, capture_output=True, check=False, timeout=300)
            log_stdout = result_regression.stdout.decode('utf-8', errors='replace')
            log_stderr = result_regression.stderr.decode('utf-8', errors='replace')
            full_log.append(f"--- Pytest Execution (Regression Tests) ---\n{log_stdout}\n{log_stderr}")
            regression_tests_passed = (result_regression.returncode == 0)
            print(f"Regression tests passed: {regression_tests_passed}")
        else:
            print("No regression tests (P2P tests) found for this instance. Setting RT% to 100%.")
            regression_tests_passed = True # å¦‚æœæ²’æœ‰ P2P æ¸¬è©¦ï¼Œå‰‡è¦–ç‚º 100% é€šé

        # 6. è¿”å›å…©å€‹çµæœ
        return f2p_passed_count, f2p_total_count, regression_tests_passed, "\n".join(full_log)

    except subprocess.TimeoutExpired:
        full_log.append("--- Pytest Execution ---\nERROR: Pytest timed out after 300 seconds.")
        return f2p_passed_count, f2p_total_count, regression_tests_passed, "\n".join(full_log)
    except Exception as e:
        full_log.append(f"--- Testing Framework Error ---\nAn unexpected error occurred: {e}")
        return f2p_passed_count, f2p_total_count, regression_tests_passed, "\n".join(full_log)


def _generate_repo_skeleton(workspace_path: str, max_files: int = 100) -> str:
    """
    æ–¹æ¡ˆä¸€ï¼šç”Ÿæˆé¡¹ç›®éª¨æ¶ (Repo Skeleton)
    åŒ…å«ï¼šæ–‡ä»¶è·¯å¾„ã€ç±»åã€å‡½æ•°åã€docstringï¼Œä½†ä¸åŒ…å«å…·ä½“å®ç°
    """
    skeleton_parts = []
    file_count = 0
    
    # è·å–æ‰€æœ‰Pythonæ–‡ä»¶
    for root, dirs, files in os.walk(workspace_path):
        # æ’é™¤å¸¸è§çš„éæ ¸å¿ƒç›®å½•
        dirs[:] = [d for d in dirs if d not in ['.git', 'venv', '__pycache__', '.tox', 'node_modules', '.pytest_cache']]
        
        if file_count >= max_files:
            break
            
        for file in files:
            if file.endswith('.py'):
                file_path = os.path.join(root, file)
                rel_path = os.path.relpath(file_path, workspace_path)
                
                # è·³è¿‡æµ‹è¯•æ–‡ä»¶
                if 'test' in rel_path.lower():
                    continue
                
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                    
                    # æå–ç±»å’Œå‡½æ•°å®šä¹‰ï¼ˆç®€åŒ–ç‰ˆéª¨æ¶ï¼‰
                    import_lines = [line for line in content.split('\n') if line.startswith('import ') or line.startswith('from ')]
                    class_lines = [line for line in content.split('\n') if line.strip().startswith('class ')]
                    func_lines = [line for line in content.split('\n') if line.strip().startswith('def ') and not line.strip().startswith('def _')]
                    
                    if import_lines or class_lines or func_lines:
                        skeleton_parts.append(f"\n--- FILE: {rel_path} ---")
                        if import_lines:
                            skeleton_parts.append("# Imports:\n" + "\n".join(import_lines[:5]))  # åªå–å‰5ä¸ª
                        if class_lines:
                            skeleton_parts.append("# Classes:\n" + "\n".join(class_lines[:10]))
                        if func_lines:
                            skeleton_parts.append("# Functions:\n" + "\n".join(func_lines[:10]))
                        
                        file_count += 1
                        
                except Exception as e:
                    continue
    
    return "\n".join(skeleton_parts)

def _keyword_search(workspace_path: str, doc_change: str) -> list[str]:
    """
    ä¿®å¤ç‰ˆï¼šåŸºäºå…³é”®è¯åœ¨ä»£ç åº“ä¸­æœç´¢ç›¸å…³æ–‡ä»¶
    """
    import re
    
    # 1. æå–å…³é”®è¯ (ä¿æŒä¸å˜)
    potential_identifiers = re.findall(r'\b[A-Z][a-zA-Z0-9]+\b|\b[a-z_][a-z0-9_]+\b', doc_change)
    common_words = {'the', 'and', 'for', 'with', 'this', 'that', 'from', 'will', 'should', 'must', 'can', 'may', 'example', 'test', 'file', 'code', 'change', 'fix', 'bug', 'issue'}
    keywords = {word for word in potential_identifiers if len(word) > 3 and word.lower() not in common_words}
    keywords = list(keywords)[:10]
    
    if not keywords:
        return []
    
    print(f"[Keyword Search] Searching for: {keywords}")
    
    matched_files = set()
    for keyword in keywords:
        try:
            # 2. æ‰§è¡Œæœç´¢ (ä¿æŒä¸å˜)
            if platform.system() == "Windows":
                result = subprocess.run(
                    ['findstr', '/S', '/M', '/I', keyword, '*.py'],
                    cwd=workspace_path,
                    capture_output=True,
                    text=True,
                    timeout=20 # å¢åŠ è¶…æ—¶æ—¶é—´
                )
            else:
                result = subprocess.run(
                    ['grep', '-r', '-l', '-i', keyword, '--include=*.py', '.'],
                    cwd=workspace_path,
                    capture_output=True,
                    text=True,
                    timeout=20
                )
            
            if result.returncode == 0:
                for line in result.stdout.split('\n'):
                    raw_line = line.strip()
                    if raw_line and 'test' not in raw_line.lower():
                        # --- å…³é”®ä¿®å¤å¼€å§‹ ---
                        # 1. ç¡®ä¿æˆ‘ä»¬æœ‰ä¸€ä¸ªç›¸å¯¹äº workspace çš„å¹²å‡€è·¯å¾„
                        # findstr/grep è¾“å‡ºé€šå¸¸å·²ç»æ˜¯ç›¸å¯¹äº cwd çš„ï¼Œä½†ä¸ºäº†ä¿é™©ï¼š
                        
                        # å¦‚æœè·¯å¾„å·²ç»æ˜¯ç»å¯¹è·¯å¾„ï¼ˆæå°‘è§ä½†å¯èƒ½ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
                        if os.path.isabs(raw_line):
                            full_path = raw_line
                        else:
                            # æ‹¼æ¥æˆç»å¯¹è·¯å¾„
                            full_path = os.path.join(workspace_path, raw_line)
                        
                        # 2. è§„èŒƒåŒ–è·¯å¾„ï¼ˆå¤„ç† .. å’Œå†—ä½™åˆ†éš”ç¬¦ï¼‰
                        full_path = os.path.normpath(full_path)
                        
                        # 3. å†æ¬¡æ£€æŸ¥è¯¥æ–‡ä»¶æ˜¯å¦çœŸçš„åœ¨ workspace å†…éƒ¨
                        # (é˜²æ­¢ç¬¦å·é“¾æ¥è·³å‡ºï¼Œæˆ–è€…ä¹‹å‰çš„è·¯å¾„è®¡ç®—é”™è¯¯)
                        if not full_path.startswith(os.path.abspath(workspace_path)):
                            continue

                        # 4. å®‰å…¨åœ°è®¡ç®—ç›¸å¯¹è·¯å¾„
                        rel_path = os.path.relpath(full_path, workspace_path)
                        
                        # 5. ç»Ÿä¸€åˆ†éš”ç¬¦ä¸º '/'
                        clean_path = rel_path.replace('\\', '/')
                        
                        # 6. å†æ¬¡è¿‡æ»¤æ‰ä»¥ .. å¼€å¤´çš„è·¯å¾„ (åŒé‡ä¿é™©)
                        if not clean_path.startswith('..'):
                            matched_files.add(clean_path)
                        # --- å…³é”®ä¿®å¤ç»“æŸ ---

        except Exception as e:
            print(f"[Keyword Search] Error searching for {keyword}: {e}")
            continue
    
    result = list(matched_files)[:5]
    print(f"[Keyword Search] Found {len(result)} files: {result}")
    return result

def _get_relevant_files_from_llm(model, doc_change: str, workspace_path: str) -> list[str]:
    """
    æ–¹æ¡ˆä¸€æ”¹è¿›ï¼šç»“åˆéª¨æ¶æœç´¢å’Œå…³é”®è¯æœç´¢
    """
    # æ–¹æ¡ˆä¸€ï¼šå…ˆè¿›è¡Œå…³é”®è¯æœç´¢
    keyword_files = _keyword_search(workspace_path, doc_change)
    
    all_files = []
    # (os.walk è¿´åœˆä¿æŒä¸è®Š)
    for root, _, files in os.walk(workspace_path):
        if '.git' in root or 'docs' in root or '.venv' in root or 'venv' in root: continue
        for file in files:
            if file.endswith(('.py', '.html', '.css', '.js', 'setup.py', 'requirements.txt')):
                rel_path = os.path.relpath(os.path.join(root, file), workspace_path)
                all_files.append(rel_path.replace('\\', '/'))
    if not all_files:
        print(f"[Task] WARNING: os.walk found NO files in {workspace_path}")
        return keyword_files  # è‡³å°‘è¿”å›å…³é”®è¯æœç´¢çš„ç»“æœ
    
    # æ–¹æ¡ˆä¸€ï¼šç”Ÿæˆéª¨æ¶
    print("[Task] Generating repository skeleton...")
    skeleton = _generate_repo_skeleton(workspace_path)
    skeleton_size = len(skeleton)
    print(f"[Task] Skeleton size: {skeleton_size} characters")
    
    file_list_str = ', '.join(all_files).replace('\\', '/')
    if not file_list_str:
        print(f"[Task] WARNING: No code files found to analyze.")
        return keyword_files

    # ğŸš€ é€™æ˜¯æ–°çš„ã€æ›´æ™ºæ…§çš„æç¤ºè©ï¼ˆåŒ…å«éª¨æ¶å’Œå…³é”®è¯æœç´¢ç»“æœï¼‰
    # åœ¨æ„å»º prompt_text å˜é‡æ—¶ï¼Œç¡®ä¿åŒ…å«ä»¥ä¸‹è¯´æ˜ï¼š
    
    system_instruction = """
    You are an expert software engineer.
    
    IMPORTANT: When you modify files, DO NOT output the entire file. 
    Output ONLY the specific code blocks that need to be changed using a SEARCH/REPLACE format.
    
    FORMAT:
    File: path/to/file.py
    <<<<
    [Original code snippet to be replaced (must match exactly)]
    ====
    [New code snippet]
    >>>>
    
    EXAMPLE:
    File: django/utils/text.py
    <<<<
    def slugify(value, allow_unicode=False):
        value = str(value)
    ====
    def slugify(value, allow_unicode=False):
        value = str(value)
        if allow_unicode:
            value = unicodedata.normalize('NFKC', value)
    >>>>
    
    RULES:
    1. Use multiple <<<< ==== >>>> blocks for multiple changes in one file.
    2. The content inside <<<< must match the original file EXACTLY (byte-for-byte), or the patch will fail.
    3. Include 2-3 lines of context around the change in the <<<< block to ensure uniqueness.
    """
    

    prompt_text = (
        f"You are an expert file locator agent. Your goal is to identify ALL files required for a code change, including dependency files.\n\n"
        f"**DOCUMENTATION CHANGE:**\n{doc_change}\n\n"
        f"**PROJECT STRUCTURE (Skeleton):**\n"
        f"{skeleton[:20000]}\n\n"  # é™åˆ¶éª¨æ¶å¤§å°
        f"**CODE FILE LIST:**\n{file_list_str}\n\n"
        f"**FILES FOUND BY KEYWORD SEARCH (High Priority):**\n"
        f"{', '.join(keyword_files) if keyword_files else 'None'}\n\n"
        f"**THINKING PROCESS (CRITICAL):**\n"
        "1.  **Core Logic:** Which file contains the primary code to be modified based on the documentation? (e.g., 'requests/models.py')\n"
        "2.  **New Symbols:** Does this change introduce new classes, functions, or exceptions? (e.g., 'JSONDecodeError')\n"
        "3.  **Definition:** Where should this new symbol be *defined*? (e.g., 'requests/exceptions.py')\n"
        "4.  **Dependencies (The most important step):**\n"
        "    - **Compatibility:** Is there a `compat.py` file that needs to be updated to handle this new symbol across different Python versions? (e.g., 'requests/compat.py')\n"
        "    - **Exporting:** Does this new symbol need to be made public? Check the relevant `__init__.py` file. (e.g., 'requests/__init__.py')\n"
        "    - **Imports:** Which other files *use* the code from step 1 and will now need to import the new symbol from step 3? (e.g., 'requests/models.py' imports from 'requests/exceptions.py')\n\n"
        f"**INSTRUCTIONS:**\n"
        "1.  Review your thinking process and list ALL files identified in steps 1-4.\n"
        "2.  Respond ONLY with a JSON object:\n"
        "   {\n"
        "     \"files\": [\"path/to/file1.py\", \"path/to/file2.py\", \"path/to/compat.py\", \"path/to/__init__.py\"]\n"
        "   }\n"
        "3.  **DO NOT** include any files from `test/` or `tests/` directories.\n"
        "   DO NOT include actions or code content in this step."
    )
    prompt = system_instruction + "\n\n" + prompt_text
    response_text = None
    try:
        response = model.generate_content(
            prompt,
            generation_config=GenerationConfig(
                response_mime_type="application/json"
            ),
            request_options=RequestOptions(timeout=300)  # 5 minutes timeout for file finding - FIXED
        )
        # (å‡½æ•¸çš„å…¶é¤˜éƒ¨åˆ†ä¿æŒä¸è®Š)
        response_text = response.text
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if not json_match:
            print(f"[Task] ERROR: AI response did not contain a JSON object. Response: {response_text}")
            return []
        json_text = json_match.group(0)
        print(f"[Task] DEBUG: File finder LLM response (extracted):\n{json_text}")
        data = json.loads(json_text)
        if "files" not in data or not isinstance(data["files"], list):
            print(f"[Task] ERROR: AI response JSON was in wrong format: {json_text}")
            return []
        llm_files = data["files"]
        valid_files = [f.strip().replace('\\', '/') for f in llm_files if f.strip() in all_files]
        
        # æ–¹æ¡ˆä¸€ï¼šå¼ºåˆ¶åŒ…å«å…³é”®è¯æœç´¢æ‰¾åˆ°çš„æ–‡ä»¶
        # for kw_file in keyword_files:
        #     if kw_file not in valid_files:
        #         valid_files.append(kw_file)
        #         print(f"[Task] Force-including keyword-matched file: {kw_file}")
        for kw_file in keyword_files:
            if kw_file not in valid_files:
                valid_files.append(kw_file)
                print(f"[Task] Force-including keyword-matched file: {kw_file}")
                
        if not valid_files and llm_files:
             print(f"[Task] WARNING: AI found files {llm_files}, but none were in the master 'all_files' list.")
        return valid_files
    except json.JSONDecodeError:
        print(f"[Task] ERROR: AI response was not valid JSON: {response_text}")
        return []
    except Exception as e:
        print(f"[Task] ERROR: Failed to parse AI file list: {e}\nResponse text: {response_text}")
        return []

def _get_file_contexts(workspace_path: str, relevant_files: list[str]) -> str:
    """
    è¯»å–ç›¸å…³æ–‡ä»¶å†…å®¹å¹¶æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
    (Reads relevant file contents and builds context string)
    
    ä¼˜åŒ–ï¼šé™åˆ¶å•ä¸ªæ–‡ä»¶å¤§å°ï¼Œé¿å…promptè¿‡å¤§
    """
    context_prompt_parts = []
    MAX_FILE_SIZE = 50000  # 50KB per file limit to avoid huge prompts
    MAX_TOTAL_SIZE = 200000  # 200KB total context limit
    total_size = 0
    
    for file_path in relevant_files:
        full_path = os.path.join(workspace_path, file_path)
        if not os.path.exists(full_path):
            print(f"WARNING: File `{file_path}` identified by AI does not exist. Skipping.")
            continue
        try:
            # Check file size before reading
            file_size = os.path.getsize(full_path)
            
            if file_size > MAX_FILE_SIZE:
                print(f"WARNING: File `{file_path}` is too large ({file_size} bytes). Truncating to first {MAX_FILE_SIZE} bytes.")
                with open(full_path, 'r', encoding='utf-8', errors='ignore') as f:
                    file_content = f.read(MAX_FILE_SIZE)
                file_content += "\n\n... [FILE TRUNCATED DUE TO SIZE] ..."
            else:
                with open(full_path, 'r', encoding='utf-8', errors='ignore') as f:
                    file_content = f.read()
            
            # Check total context size
            content_block = (
                f"--- START OF FILE: {file_path} ---\n"
                f"{file_content}\n"
                f"--- END OF FILE: {file_path} ---\n"
            )
            
            if total_size + len(content_block) > MAX_TOTAL_SIZE:
                print(f"WARNING: Total context size would exceed {MAX_TOTAL_SIZE} bytes. Skipping remaining files.")
                context_prompt_parts.append(
                    f"\n... [REMAINING FILES SKIPPED DUE TO CONTEXT SIZE LIMIT] ...\n"
                )
                break
            
            context_prompt_parts.append(content_block)
            total_size += len(content_block)
            
        except Exception as e:
            print(f"Error reading file {file_path}: {e}")
    
    result = "\n".join(context_prompt_parts)
    print(f"[Context] Total context size: {len(result)} characters ({len(relevant_files)} files)")
    return result

def _parse_v7_response(raw_response_text: str) -> dict[str, str]:
    """
    (æ­¤å‡½æ•¸ä¿æŒä¸è®Š)
    (This function is unchanged)
    """
    pattern = re.compile(
        r"File:\s*(.*?)\n\s*<<<<\n(.*?)\n====\n(.*?)\n>>>>", 
        re.DOTALL
    )

    matches = pattern.findall(raw_response_text)
    
    if not matches:
        # å¦‚æœæ²’æœ‰åŒ¹é…åˆ°å—æ ¼å¼ï¼Œå˜—è©¦å›é€€åˆ°èˆŠçš„ "å…¨æ–‡ä»¶é‡å¯«" æ ¼å¼æ£€æŸ¥
        if "--- START OF FILE:" in raw_response_text:
            print("WARNING: Falling back to legacy FULL FILE parsing.")
            file_chunks = re.split(r'--- START OF FILE: (.*?) ---\n', raw_response_text)
            for i in range(1, len(file_chunks), 2):
                file_path = file_chunks[i].strip()
                content_part = file_chunks[i+1]
                content = re.sub(r'--- END OF FILE: .*? ---', '', content_part, flags=re.DOTALL).strip()
                modified_files[file_path] = content
            return modified_files
        
        print("WARNING: No valid code blocks found in AI response.")
        return {}
    file_changes = {}
    for file_path, search_block, replace_block in matches:
        file_path = file_path.strip()
        if file_path not in file_changes:
            file_changes[file_path] = []
        file_changes[file_path].append((search_block, replace_block))

    for file_path, changes in file_changes.items():
        pass
    pass
    raise NotImplementedError("ç”±äºæ¶æ„é™åˆ¶ï¼Œè¯·ä½¿ç”¨ä¸‹é¢çš„ 'æ–¹æ¡ˆ B' ä¿®æ”¹ run_agent_attempt")


# --- æŒ‡æ¨™è¨ˆç®— (Metrics Calculation) ---
# (parse_patch å’Œ calculate_f1_score ä¿æŒä¸è®Š)
def parse_patch(patch_str: str) -> dict[str, set[int]]:
    if not patch_str: return {}
    try:
        patch = PatchSet(StringIO(patch_str))
        changed_files = {}
        for patched_file in patch:
            if patched_file.is_binary_file: continue
            file_path = patched_file.target_file[2:] if patched_file.target_file.startswith('b/') else patched_file.target_file
            changed_lines = set()
            for hunk in patched_file:
                for line in hunk:
                    if line.is_added:
                        changed_lines.add(line.target_line_no)
                    elif line.is_removed:
                        changed_lines.add(line.source_line_no)
            if changed_lines:
                changed_files[file_path] = changed_lines
        return changed_files
    except Exception as e:
        print(f"Error parsing patch: {e}\nPatch content:\n{patch_str[:500]}...")
        return {}

def calculate_f1_score(pred_set: set, gold_set: set) -> float:
    if not gold_set: return 1.0 if not pred_set else 0.0
    if not pred_set: return 0.0
    tp = len(pred_set.intersection(gold_set))
    fp = len(pred_set.difference(gold_set))
    fn = len(gold_set.difference(pred_set))
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    return f1

def calculate_all_metrics(
    f2p_passed_count: int,
    f2p_total_count: int,
    regression_tests_passed: bool,
    applied_successfully: bool, 
    generated_patch: str, 
    ground_truth_patch: str, 
    run_time_seconds: float
) -> dict:
    """
    (æ­¤å‡½æ•¸èˆ‡ V14/V19 ç‰ˆæœ¬å®Œå…¨ç›¸åŒ)
    (This function is identical to the V14/V19 version)
    """
    
    # 1. Success% å’Œ RT%
    success_percent = 100.0 if (f2p_passed_count == f2p_total_count and f2p_total_count > 0) else 0.0
    rt_percent = 100.0 if regression_tests_passed else 0.0
    applied_percent = 100.0 if applied_successfully else 0.0

    # 2. FV-Macro (æ¯å€‹å¯¦ä¾‹)
    fv_macro = 100.0 * (f2p_passed_count / f2p_total_count) if f2p_total_count > 0 else 0.0

    # 3. File% (ç²¾ç¢ºç‡)
    pred_files_lines = parse_patch(generated_patch)
    gold_files_lines = parse_patch(ground_truth_patch)
    pred_file_set = set(pred_files_lines.keys())
    gold_file_set = set(gold_files_lines.keys())
    
    file_intersection = len(pred_file_set.intersection(gold_file_set))
    if len(pred_file_set) == 0:
        file_percent = 100.0 if len(gold_file_set) == 0 else 0.0
    else:
        file_percent = (file_intersection / len(pred_file_set)) * 100.0

    return {
        'success_percent': success_percent,
        'applied_percent': applied_percent,
        'rt_percent': rt_percent,
        'fv_macro': fv_macro,
        'file_percent': file_percent,
        'num_token': len(generated_patch.split()),
        'run_time_seconds': run_time_seconds,
        'f2p_passed_count': f2p_passed_count,
        'f2p_total_count': f2p_total_count,
    }

# --- æ ¸å¿ƒ Agent å·¥ä½œå‡½æ•¸ (Core Agent Worker Function) ---

def run_agent_attempt_with_reflexion(
    workspace_path: str,
    model,
    initial_prompt: str,
    feature_test_patch: str,
    f2p_test_names: list[str],
    p2p_test_names: list[str],
    max_reflexion_iterations: int = 3  # æ–¹æ¡ˆäºŒï¼šæœ€å¤§è‡ªæ ¡æ­£æ¬¡æ•°
) -> dict:
    """
    æ–¹æ¡ˆäºŒï¼šå®ç° Reflexion (Self-Correction Loop)
    è¿è¡Œ Agent å¹¶æ ¹æ®æµ‹è¯•å¤±è´¥è¿›è¡Œè‡ªæˆ‘è°ƒè¯•å’Œé‡è¯•
    """
    print(f"[Reflexion] Starting with max {max_reflexion_iterations} iterations...")
    
    for iteration in range(max_reflexion_iterations):
        print(f"\n[Reflexion] === Iteration {iteration + 1}/{max_reflexion_iterations} ===")
        
        # æ„å»ºå½“å‰è¿­ä»£çš„ prompt
        if iteration == 0:
            current_prompt = initial_prompt
        else:
            # åœ¨åç»­è¿­ä»£ä¸­ï¼Œæ·»åŠ ä¸Šä¸€æ¬¡çš„é”™è¯¯ä¿¡æ¯
            current_prompt = (
                f"{initial_prompt}\n\n"
                f"**PREVIOUS ATTEMPT FAILED**\n"
                f"Your previous code changes did not pass the tests. Here is the test output:\n\n"
                f"```\n{last_test_output}\n```\n\n"
                f"**INSTRUCTIONS FOR RETRY:**\n"
                f"1. Carefully analyze the test failure messages above\n"
                f"2. Identify what went wrong in your previous implementation\n"
                f"3. Generate corrected code that fixes these specific errors\n"
                f"4. Make sure to address ALL failing tests\n"
                f"5. Do NOT repeat the same mistakes\n"
            )
        
        # è¿è¡Œä¸€æ¬¡å°è¯•
        result = run_agent_attempt(
            workspace_path=workspace_path,
            model=model,
            prompt_text=current_prompt,
            feature_test_patch=feature_test_patch,
            f2p_test_names=f2p_test_names,
            p2p_test_names=p2p_test_names
        )
        
        # æ£€æŸ¥ç»“æœ
        status = result.get('status', '')
        
        # æ–¹æ¡ˆä¸‰ï¼šç¯å¢ƒé”™è¯¯ä¸é‡è¯•
        if status == 'ENV_ERROR':
            print(f"[Reflexion] Environment error detected. Cannot retry.")
            return result
        
        # å¦‚æœé€šè¿‡æµ‹è¯•ï¼Œè¿”å›æˆåŠŸ
        if status == 'PASSED':
            print(f"[Reflexion] SUCCESS after {iteration + 1} iteration(s)!")
            result['reflexion_iterations'] = iteration + 1
            return result
        
        # å¦‚æœæ˜¯æœ€åä¸€æ¬¡è¿­ä»£ï¼Œè¿”å›å¤±è´¥
        if iteration == max_reflexion_iterations - 1:
            print(f"[Reflexion] FAILED after {max_reflexion_iterations} iterations.")
            result['reflexion_iterations'] = max_reflexion_iterations
            return result
        
        # å‡†å¤‡ä¸‹ä¸€æ¬¡è¿­ä»£
        last_test_output = result.get('test_output', 'No test output available')
        print(f"[Reflexion] Test failed. Preparing retry with error feedback...")
    
    # ä¸åº”è¯¥åˆ°è¾¾è¿™é‡Œ
    return result


def run_agent_attempt(
    workspace_path: str, 
    model, 
    prompt_text: str, 
    feature_test_patch: str,  # ğŸš€ æ–°å¢ (NEW)
    f2p_test_names: list[str],  # ğŸš€ æ–°å¢ (NEW)
    p2p_test_names: list[str]   # ğŸš€ æ–°å¢ (NEW)
) -> dict:
    """
    é‹è¡Œä¸€æ¬¡ Agent å˜—è©¦ï¼šé‡ç½®ã€ç·¨ç¢¼ã€å¯«å…¥ã€å·®ç•°æ¯”è¼ƒã€æ¸¬è©¦ã€‚
    (Runs one agent attempt: reset, code, write, diff, test.)
    æ³¨æ„ï¼šè¿™ä¸ªå‡½æ•°ç°åœ¨ç”± run_agent_attempt_with_reflexion è°ƒç”¨
    """
    
    raw_response_text = ""
    final_patch_str = ""
    test_output = ""
    
    try:
        # é‡ç½®å·¥ä½œå€ (Reset workspace)
        subprocess.run(['git', 'reset', '--hard', 'HEAD'], cwd=workspace_path, capture_output=True, text=True, check=True)
        
        # 1. ç”Ÿæˆç¨‹å¼ç¢¼ (Generate Code) with timeout and retry
        max_retries = 1  # Increased from 2 to 3
        retry_count = 0
        response = None
        last_error = None
        
        while retry_count < max_retries:
            try:
                prompt_size = len(prompt_text)
                print(f"[Task] Calling Gemini API (attempt {retry_count + 1}/{max_retries})...")
                print(f"[Task] Prompt size: {prompt_size} characters (~{prompt_size/4:.0f} tokens)")
                response = model.generate_content(
                    prompt_text,
                    request_options=RequestOptions(timeout=300)  # 15 minutes timeout - FIXED: using RequestOptions object
                )
                raw_response_text = response.text
                break  # Success, exit retry loop
            except Exception as api_error:
                last_error = api_error
                retry_count += 1
                error_str = str(api_error)
                
                if "504" in error_str or "Deadline Exceeded" in error_str:
                    if retry_count < max_retries:
                        wait_time = 5 * retry_count  # Reduced wait time (5s instead of 10s)
                        print(f"[Task] API timeout (504). Retrying in {wait_time}s... (attempt {retry_count}/{max_retries})")
                        time.sleep(wait_time)
                    else:
                        print(f"[Task] API timeout after {max_retries} attempts. Giving up.")
                        return {
                            'status': 'APPLY_FAILED', 
                            'error': f"Gemini API timeout after {max_retries} attempts: {error_str}",
                            'patch': '', 'raw_response': '',
                            'f2p_passed_count': 0, 'f2p_total_count': 0, 'regression_tests_passed': False
                        }
                else:
                    # Non-timeout error, don't retry
                    print(f"[Task] API error (non-timeout): {error_str}")
                    return {
                        'status': 'APPLY_FAILED', 
                        'error': f"Gemini API error: {error_str}",
                        'patch': '', 'raw_response': '',
                        'f2p_passed_count': 0, 'f2p_total_count': 0, 'regression_tests_passed': False
                    }
        
        if response is None:
            return {
                'status': 'APPLY_FAILED', 
                'error': f"Failed to get API response after {max_retries} attempts",
                'patch': '', 'raw_response': '',
                'f2p_passed_count': 0, 'f2p_total_count': 0, 'regression_tests_passed': False
            }
        
        # 2. è§£æå›æ‡‰ (Parse Response)
        try:
            modified_files = _parse_v7_response(raw_response_text)
        except Exception as e:
            print(f"ERROR: AI response parsing failed: {e}\nRaw Response: {raw_response_text[:1000]}")
            return {
                'status': 'APPLY_FAILED', 'error': f"AI response parsing failed: {e}",
                'patch': '', 'raw_response': raw_response_text,
                'f2p_passed_count': 0, 'f2p_total_count': 0, 'regression_tests_passed': False
            }

        # 3. å°‡æ–°å…§å®¹å¯«å…¥æ–‡ä»¶ (Write new contents to files)
        for file_path, new_content in modified_files.items():
            try:
                if '..' in file_path: continue
                full_path = os.path.join(workspace_path, file_path)
                os.makedirs(os.path.dirname(full_path), exist_ok=True)
                with open(full_path, 'w', encoding='utf-8') as f:
                    f.write(new_content)
            except Exception as e:
                 return {
                    'status': 'APPLY_FAILED', 'error': f"Failed to write file {file_path} to disk: {e}",
                    'patch': '', 'raw_response': raw_response_text,
                    'f2p_passed_count': 0, 'f2p_total_count': 0, 'regression_tests_passed': False
                }

        # 4. ç”Ÿæˆè£œä¸ (Generate Patch)
        diff_result = subprocess.run(
            ['git', 'diff', '--no-prefix'], 
            cwd=workspace_path, capture_output=True, text=True, check=True, encoding='utf-8'
        )
        final_patch_str = diff_result.stdout
        
        if not final_patch_str:
             return {
                'status': 'TEST_FAILED', 'patch': '',
                'test_output': 'AI agent produced no code changes.',
                'raw_response': raw_response_text,
                'f2p_passed_count': 0, 'f2p_total_count': 0,
                'regression_tests_passed': False,
            }

        # 5. ğŸš€ æ›´æ”¹ (CHANGE): é‹è¡Œå…©ç¨®æ¸¬è©¦
        # (Run both test types)
        f2p_passed_count, f2p_total_count, regression_tests_passed, test_output = _run_tests_in_workspace(
            workspace_path, 
            feature_test_patch,
            f2p_test_names,
            p2p_test_names
        )
        
        # æ–¹æ¡ˆä¸‰ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯ç¯å¢ƒé”™è¯¯
        if test_output.startswith("ENV_ERROR:"):
            return {
                'status': 'ENV_ERROR',
                'error': test_output,
                'patch': final_patch_str,
                'test_output': test_output,
                'raw_response': raw_response_text,
                'f2p_passed_count': 0,
                'f2p_total_count': f2p_total_count,
                'regression_tests_passed': False,
            }
        
        if f2p_total_count > 0 and f2p_passed_count == f2p_total_count:
            return {
                'status': 'PASSED', 'patch': final_patch_str,
                'test_output': test_output, 'raw_response': raw_response_text,
                'f2p_passed_count': f2p_passed_count,
                'f2p_total_count': f2p_total_count,
                'regression_tests_passed': regression_tests_passed,
            }
        else:
            return {
                'status': 'TEST_FAILED', 'patch': final_patch_str,
                'test_output': test_output, 'raw_response': raw_response_text,
                'f2p_passed_count': f2p_passed_count,
                'f2p_total_count': f2p_total_count,
                'regression_tests_passed': regression_tests_passed,
            }
        
    except Exception as e:
        print(f"FATAL ERROR in run_agent_attempt: {e}")
        return {
            'status': 'APPLY_FAILED',
            'error': f"An unexpected error occurred in the agent worker: {e}",
            'patch': final_patch_str,
            'test_output': test_output,
            'raw_response': raw_response_text,
            'f2p_passed_count': 0, 'f2p_total_count': 0, 'regression_tests_passed': False
        }

def setup_custom_workspace(github_url: str) -> str:
    """
    å¾ä¸€å€‹ Git URL è¤‡è£½ä¸¦åˆå§‹åŒ–ä¸€å€‹å·¥ä½œå€ã€‚
    """
    run_id = str(time.time()).replace('.', '')
    # ç”¢ç”Ÿä¸€å€‹å”¯ä¸€çš„ç›®éŒ„åç¨±
    repo_name = github_url.split('/')[-1].replace('.git', '')
    temp_dir = os.path.join(ROOT_WORKSPACE, f'demo_{repo_name}_{run_id}')
    
    try:
        # è¤‡è£½ Git å€‰åº«
        print(f"Cloning repo from {github_url} into {temp_dir}...")
        subprocess.run(
            ['git', 'clone', '--depth', '1', github_url, temp_dir],
            check=True, capture_output=True, text=True, encoding='utf-8'
        )
        
        # (å¯é¸ï¼Œä½†æ¨è–¦) åˆå§‹åŒ– Gitï¼Œä»¥ä¾¿æˆ‘å€‘å¯ä»¥ 'git diff'
        subprocess.run(['git', 'init'], cwd=temp_dir, check=True, capture_output=True, text=True, encoding='utf-8')
        subprocess.run(['git', 'add', '.'], cwd=temp_dir, check=True, capture_output=True, text=True, encoding='utf-8')
        subprocess.run(['git', 'commit', '-m', 'Initial snapshot', '--allow-empty'], cwd=temp_dir, check=True, capture_output=True, text=True, encoding='utf-8')
        print(f"Workspace initialized at {temp_dir}")
        return temp_dir
        
    except subprocess.CalledProcessError as e:
        raise IOError(f"Failed to clone Git repo: {e.stderr}")
    except Exception as e:
        raise IOError(f"File operation failed: {e}")

# ... (åœ¨ run_agent_attempt æ—é‚Š)

def run_agent_demo_attempt(
    workspace_path: str, 
    model, 
    prompt_text: str
) -> dict:
    """
    é‹è¡Œä¸€æ¬¡ Agent å˜—è©¦ï¼Œä½† *ä¸åŸ·è¡Œ* ä»»ä½•æ¸¬è©¦ã€‚
    (Runs one agent attempt, but does *not* run tests.)
    """
    
    raw_response_text = ""
    final_patch_str = ""
    
    try:
        # é‡ç½®å·¥ä½œå€
        subprocess.run(['git', 'reset', '--hard', 'HEAD'], cwd=workspace_path, capture_output=True, text=True, check=True)
        
        # 1. ç”Ÿæˆç¨‹å¼ç¢¼ with timeout
        response = model.generate_content(
            prompt_text,
            request_options=RequestOptions(timeout=300)  # 5 minutes timeout - FIXED
        )
        raw_response_text = response.text
        
        # 2. è§£æå›æ‡‰
        try:
            modified_files = _parse_v7_response(raw_response_text)
        except Exception as e:
            print(f"ERROR: AI response parsing failed: {e}")
            return {'status': 'APPLY_FAILED', 'patch': '', 'raw_response': raw_response_text}

        # 3. å°‡æ–°å…§å®¹å¯«å…¥æ–‡ä»¶
        for file_path, new_content in modified_files.items():
            try:
                if '..' in file_path: continue
                full_path = os.path.join(workspace_path, file_path)
                os.makedirs(os.path.dirname(full_path), exist_ok=True)
                with open(full_path, 'w', encoding='utf-8') as f:
                    f.write(new_content)
            except Exception as e:
                 return {'status': 'APPLY_FAILED', 'patch': '', 'raw_response': raw_response_text}

        # 4. ç”Ÿæˆè£œä¸
        diff_result = subprocess.run(
            ['git', 'diff', '--no-prefix'], 
            cwd=workspace_path, capture_output=True, text=True, check=True, encoding='utf-8'
        )
        final_patch_str = diff_result.stdout
        
        # 5. æˆåŠŸè¿”å› (ä¸é‹è¡Œæ¸¬è©¦)
        return {
            'status': 'COMPLETED', # ç‹€æ…‹ç¸½æ˜¯ COMPLETEDï¼Œå› ç‚ºæ²’æœ‰æ¸¬è©¦
            'patch': final_patch_str,
            'raw_response': raw_response_text,
        }
        
    except Exception as e:
        print(f"FATAL ERROR in run_agent_demo_attempt: {e}")
        return {'status': 'APPLY_FAILED', 'patch': '', 'raw_response': raw_response_text}

